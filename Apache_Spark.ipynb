{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtYDsT1RDw5wSxy05YHKNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beejhay31/ML-With-Apache/blob/main/Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4AB9AXI1hh_",
        "outputId": "a796da69-5a05-4fc6-8b00-0cd231b70225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "iUF_NPEl1lwV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-zF8SwCJ19kH",
        "outputId": "815e9b63-f6df-4d20-b751-f9b457d51267"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.2.0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loading important package of spark \n",
        "\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.pipeline import Transformer,Estimator\n",
        "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ],
      "metadata": {
        "id": "BdhCkrj4cmCj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Titanic Data') \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "qYSHSFDs2KlA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load data function for loading data..\n",
        "@param - \n",
        "        path - path of file\n",
        "        header_value - header value, incase true first row will be header\n",
        "        \n",
        "@return - dataframe of loaded intended data.\n",
        "\"\"\"\n",
        "\n",
        "def load_data(path,header_value):\n",
        "  df = spark.read.csv(path,inferSchema=True,header=header_value)\n",
        "  return df\n",
        "df = load_data('train.csv',True) \n",
        "df_test = load_data('test.csv',True)\n"
      ],
      "metadata": {
        "id": "EliGHYYJcpUr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Custom Transformer class for tranformation implementation .\n",
        "\n",
        "@param - \n",
        "       Transformer - Transformer class refrence \n",
        "       df - dataframe in which operation need to be carried ( passed through tranform function)\n",
        "       A - A class for variable sharing.\n",
        "\n",
        "@return -\n",
        "       df - a dataframe which contains prediction value as well with featured value. \n",
        "\n",
        "'''\n",
        "\n",
        "class preprocess_transform(Transformer):\n",
        "  \n",
        "    def _transform(self,df):\n",
        "      print(\"********************************  in Transform method ...************************************\")\n",
        "      \n",
        "      \n",
        "      \"\"\"\n",
        "      Generate feature column in dataframe based on specific logic\n",
        "\n",
        "      @param - \n",
        "               df - dataframe for operation.\n",
        "\n",
        "      @return - \n",
        "               df - dataframe with generated feature.\n",
        "      \"\"\"\n",
        "      \n",
        "      \n",
        "      def feature_generation(self,df):\n",
        "        df = df.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
        "        df = df.replace(['Mlle','Mme', 'Ms', 'Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
        "                        ['Miss','Miss','Miss','Mr','Mr',  'Mrs',  'Mrs',  'Other',  'Other','Other','Mr','Mr','Mr'])\n",
        "        df = df.withColumn(\"Family_Size\",col('SibSp')+col('Parch'))\n",
        "        #df = df.withColumn('Alone',lit(0))\n",
        "        #df = df.withColumn(\"Alone\",when(df[\"Family_Size\"] ==0, 1).otherwise(df[\"Alone\"]))\n",
        "        return df\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      Impute Age based on Age mean of specific gender. ex for male mean is 46 update all null male row with 46, similarly for others\n",
        "\n",
        "      @param - \n",
        "              df - dataframe for operation\n",
        "\n",
        "      @return -\n",
        "             df - with imputed value\n",
        "\n",
        "      \"\"\"\n",
        "  \n",
        "      def Age_impute(self,df):\n",
        "        Age_mean = df.groupBy(\"Initial\").avg('Age')\n",
        "        Age_mean = Age_mean.withColumnRenamed('avg(Age)','mean_age')\n",
        "        Initials_list = Age_mean.select(\"Initial\").rdd.flatMap(lambda x: x).collect()\n",
        "        Mean_list = Age_mean.select(\"mean_age\").rdd.flatMap(lambda x: x).collect()\n",
        "        for i,j in zip(Initials_list,Mean_list):\n",
        "            df = df.withColumn(\"Age\",when((df[\"Initial\"] == i) & (df[\"Age\"].isNull()), j).otherwise(df[\"Age\"]))\n",
        "\n",
        "        return df\n",
        "        \n",
        "        \n",
        "      \"\"\"\n",
        "      Impute Embark based on mode of embark column\n",
        "      @param - \n",
        "              df - dataframe for operation\n",
        "\n",
        "      @return -\n",
        "             df - with imputed value\n",
        "\n",
        "      \"\"\"\n",
        "      def Embark_impute(self,df):\n",
        "        mode_value = df.groupBy('Embarked').count().sort(col('count').desc()).collect()[0][0]\n",
        "        df = df.fillna({'Embarked':mode_value})\n",
        "        return df\n",
        "      \n",
        "      \n",
        "      \"\"\"\n",
        "      Impute Fare based on the class which he/she had sat ex: class 3rd has mean fare 9 and null fare belong to 3rd class so fill 9\n",
        "      @param - \n",
        "              df - dataframe for operation\n",
        "\n",
        "      @return -\n",
        "             df - with imputed value\n",
        "\n",
        "      \"\"\"\n",
        "      def Fare_impute(self,df):\n",
        "        Select_pclass = df.filter(col('Fare').isNull()).select('Pclass')\n",
        "        if Select_pclass.count() > 0:\n",
        "          Pclass = Select_pclass.rdd.flatMap(lambda x: x).collect()\n",
        "          for i in Pclass:\n",
        "            mean_pclass_fare = df.groupBy('Pclass').mean().select('Pclass','avg(Fare)').filter(col('Pclass')== i).collect()[0][1]\n",
        "            df = df.withColumn(\"Fare\",when((col('Fare').isNull()) & (col('Pclass') == i),mean_pclass_fare).otherwise(col('Fare')))\n",
        "        return df\n",
        "      \n",
        "      \n",
        "      '''\n",
        "      combining all column imputation together..\n",
        "\n",
        "      @param - \n",
        "            df - a dataframe for operation.\n",
        "\n",
        "      @return - \n",
        "            df - dataframe with imputed value.\n",
        "\n",
        "      '''\n",
        "      def all_impute_together(df):\n",
        "        df = Age_impute(self,df)\n",
        "        df = Embark_impute(self,df)\n",
        "        df = Fare_impute(self,df)\n",
        "        return df\n",
        "      \n",
        "      \n",
        "      '''\n",
        "      converting string to numeric values.\n",
        "\n",
        "      @param - \n",
        "               df - dataframe contained all columns.\n",
        "               col_list - list of column need to be \n",
        "\n",
        "      @return - \n",
        "              df - transformed dataframe.\n",
        "      '''\n",
        "      def stringToNumeric_conv(df,col_list):\n",
        "        indexer = [StringIndexer(inputCol=column,outputCol=column+\"_index\").fit(df) for column in col_list]\n",
        "        string_change_pipeline = Pipeline(stages=indexer)\n",
        "        df = string_change_pipeline.fit(df).transform(df)\n",
        "        return df\n",
        "\n",
        "      \n",
        "      \"\"\"\n",
        "      Drop column from dataframe\n",
        "      @param -\n",
        "             df - dataframe \n",
        "             col_name - name of column which need to be dropped.\n",
        "      @return -\n",
        "             df - a dataframe except dropped column\n",
        "      \"\"\"\n",
        "      def drop_column(df,col_list):\n",
        "        for i in col_list:\n",
        "            df = df.drop(col(i))\n",
        "        return df\n",
        "      \n",
        "      \n",
        "      col_list = [\"Sex\",\"Embarked\", \"Initial\"]\n",
        "      dataset = feature_generation(self,df)\n",
        "      df_impute = all_impute_together(dataset)\n",
        "      df_numeric = stringToNumeric_conv(df_impute,col_list)\n",
        "      df_final = drop_column(df_numeric,['Cabin','Name','Ticket','Family_Size','SibSp','Parch','Sex','Embarked', 'Initial'])\n",
        "      return df_final"
      ],
      "metadata": {
        "id": "7yPKfv5qcpaX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-C8k7HytQiN",
        "outputId": "0f5b9d79-bee0-47b3-89ff-7463c4ee5303"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.7/dist-packages (1.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.19.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.1)\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow) (20.1.0)\n",
            "Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.16.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.2)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (5.0.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.18.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (4.10.0)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.1.26)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.29)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.7.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (21.3)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=4.0.0->mlflow) (1.2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow) (1.1.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow) (5.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mlflow) (2.8.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow"
      ],
      "metadata": {
        "id": "MqaTVUhwuY98"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier \n",
        "from pyspark.ml.classification import RandomForestClassifier \n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator  \n",
        "\n",
        "# initialization for pipeline setup \n",
        "my_model = preprocess_transform() \n",
        "df = my_model.transform(df) \n",
        "feature = VectorAssembler(inputCols=['Pclass','Age','Fare','Sex_index','Embarked_index'],outputCol=\"features\")  \n",
        "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\", numTrees=10) \n",
        "\n",
        "''' pipeline stages initilization , fit and transform. '''\n",
        "\n",
        "pipeline = Pipeline(stages=[feature,rf])  \n",
        "model = pipeline.fit(df)  \n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees,[100,300]).build()  \n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")  \n",
        "crossval = CrossValidator(estimator=pipeline,                           estimatorParamMaps=paramGrid,                        evaluator=evaluator,numFolds=3)  \n",
        "# use 3+ folds in practice  # Run cross-validation, and choose the best set of parameters. \n",
        "cvModel = crossval.fit(df)\n",
        "df_test = my_model.transform(df_test) \n",
        "prediction = cvModel.transform(df_test)   \n",
        "mlflow.spark.log_model(model, \"spark-model16\") \n",
        "mlflow.spark.save_model(model, \"spark-model_test\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmVcKLaTcpcC",
        "outputId": "125ae5e4-206a-49d1-8443-f371f07b6b85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************  in Transform method ...************************************\n",
            "********************************  in Transform method ...************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask\n",
        "#from sklearn.externals import joblib\n",
        "from flask import Flask, jsonify, request\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "#import preprocess_file\n",
        "#import train_model\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row,SQLContext\n",
        "from collections import OrderedDict\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "st = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName('Titanic') \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sqlContext = SQLContext(st.sparkContext)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template(\"home.html\")\n",
        "\n",
        "@app.route('/')\n",
        "def train_model_fun():\n",
        "    train_model.training_stage()\n",
        "    return 'model_train is done'\n",
        "\n",
        "@app.route('/predict',methods=['POST'])\n",
        "def predict_func():\n",
        "    test_data = request.get_json()\n",
        "    \n",
        "    # test_df = pd.DataFrame.from_dict(,orient='index')\n",
        "    # test_df = test_df.T\n",
        "    new_df = sqlContext.read.json(st.sparkContext.parallelize(test_data))\n",
        "    model = PipelineModel.load('spark-model_test')\n",
        "    process_data = preprocess_transform().transform(new_df)\n",
        "    predicted_data = model.transform(process_data)\n",
        "    #print(\"predicted_data\",predicted_data.show())\n",
        "    print(\"my prediction\",predicted_data.select(\"PassengerId\",\"prediction\").show())\n",
        "    output = {'PassengerId':predicted_data.select(\"PassengerId\").collect()[0][0],'Survived':predicted_data.select(\"prediction\").collect()[0][0]}\n",
        "    print(output)\n",
        "    return jsonify(output)\n",
        "\n",
        "cols = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template(\"home.html\")\n",
        "\n",
        "@app.route('/predict',methods=['POST'])\n",
        "def predict():\n",
        "    int_features = [x for x in request.form.values()]\n",
        "    final = np.array(int_features)\n",
        "    data_unseen = pd.DataFrame([final], columns = cols)\n",
        "    prediction = predict_model(model, data=data_unseen, round = 0)\n",
        "    prediction = int(prediction.Label[0])\n",
        "    return render_template('home.html',pred='Expected Bill will be {}'.format(prediction))\n",
        "\n",
        "@app.route('/predict_api',methods=['POST'])\n",
        "def predict_api():\n",
        "    data = request.get_json(force=True)\n",
        "    data_unseen = pd.DataFrame([data])\n",
        "    prediction = predict_model(model, data=data_unseen)\n",
        "    output = prediction.Label[0]\n",
        "    return jsonify(output)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4YDCl8ecpgy",
        "outputId": "07623970-262e-476d-ebd2-13ebab8c3ef8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "eFm3xvDtcpjf",
        "outputId": "c43599f7-2046-4592-852c-f49435bc2b07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-68891c5c0f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flask/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, host, port, debug, load_dotenv, **options)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mrun_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0;31m# reset the first request information if the development server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/werkzeug/serving.py\u001b[0m in \u001b[0;36mrun_simple\u001b[0;34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress_family\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOL_SOCKET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSO_REUSEADDR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_inheritable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_inheritable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 98] Address already in use"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bfG-6SKM0Gdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}